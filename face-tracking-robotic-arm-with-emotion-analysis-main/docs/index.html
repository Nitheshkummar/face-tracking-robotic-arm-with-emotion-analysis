<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ROBxMFC - Face Tracking Robotic Arm</title>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600;700&display=swap" rel="stylesheet">
    <style>
        /* General Reset */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Poppins', sans-serif; /* Modern and clean font */
        }
        
        body {
            background-color: #222831; /* Dark background */
            color: #EEEEEE; /* Light text for contrast */
            overflow-x: hidden;
        }

        /* Smooth scrolling for anchor links */
        html {
            scroll-behavior: smooth;
        }
        
        /* Navbar */
        .nav {
            position: fixed;
            top: 0;
            width: 100%;
            padding: 1.5rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
            z-index: 100;
            background-color: rgba(34, 40, 49, 0.95); /* Slightly transparent dark background */
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);
        }
        
        .nav-left {
            font-size: 1.8rem;
            font-weight: 700;
            color: #00ADB5; /* Accent color */
        }
        
        .nav-right {
            display: flex;
            gap: 1.5rem;
            align-items: center;
        }
        
        .nav-right a {
            color: #EEEEEE;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        
        .nav-right a:hover {
            color: #00ADB5; /* Accent color on hover */
        }
        
        /* Project Header */
        .project-header {
            padding-top: 120px;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            text-align: center;
            padding: 6rem 2rem 2rem;
            
        }

        .project-title {
            font-size: 4rem;
            line-height: 1.2;
            margin-top: 10rem;
            margin-bottom: 3rem;
            text-transform: uppercase;
            color: #00ADB5; /* Accent color */
        }
                
        .logo-container {
            margin-top: 4em;
            margin-bottom: rem;
            max-width: 300px;
        }
        
        .logo-container img {
            width: 100%;
            height: auto;
            border-radius: 10px; /* Rounded edges for the logo */
        }
        
        /* Course Info */
        .course-info {
            display: flex;
            gap: 2rem;
            justify-content: center;
            flex-wrap: wrap;
            margin-top: 2rem;
        }
        
        
    .course-card {
        background-color: rgba(57, 62, 70, 0.8); /* Slightly transparent dark card */
        padding: 1.5rem;
        border-radius: 8px;
        max-width: 350px;
        text-align: left;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);
        position: absolute; /* Position the card absolutely */
        z-index:-100; /* Ensure it stays above other elements */
    }

    /* Top-left card */
    .course-card.topleft {
        top: 10em; /* Distance from the top */
        left: 10rem;/* Distance from the left */
    }









    /* Top-right card */
    .course-card.topright {
        top: 10em; /* Distance from the top */
        right: 10rem;/* Distance from the right */
    }

    .course-card h3 {
        font-size: 1.5rem;
        margin-bottom: 0.5rem;
        color: #00ADB5; /* Accent color */
    }

    .course-card p {
        font-size: 1.1rem;
        opacity: 0.9;
        color: #EEEEEE;
    }
        
        .course-card p {
            font-size: 1.1rem;
            opacity: 0.9;
            color: #EEEEEE;
        }
        
        @media (max-width: 768px) {
    .course-card {
        display: none;
    }
}

        /* Abstract Section */
        #abstract {
            padding: 4rem 2rem;
            background: rgba(57, 62, 70, 0.8); /* Glassmorphism effect */
            color: #EEEEEE;
            text-align: center;
            border-radius: 15px;
            backdrop-filter: blur(15px);
            -webkit-backdrop-filter: blur(15px);
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5);
            border: 1px solid rgba(255, 255, 255, 0.2);
            margin: 2rem auto;
            max-width: 800px;
        }

        #abstract h2 {
            font-size: 2.5rem;
            margin-bottom: 1.5rem;
            color: #00ADB5; /* Accent color */
        }

        #abstract p {
            font-size: 1.2rem;
            line-height: 1.6;
        }
        
        /* Responsive Design */
        @media (max-width: 768px) {
            .project-title {
                font-size: 2.5rem;
            }
            
            .course-info {
                flex-direction: column;
                align-items: center;
            }
        }

        
    </style>
</head>
<body>
    <nav class="nav">
        <div class="nav-left">
            ROBxMFC
        </div>
        <div class="nav-right">
            <a href="#abstract">Abstract</a>
            <a href="#introduction">Introduction</a>
            <a href="#literature-review">Literature Review</a>
            <a href="#methodology">Methodology</a>
            <a href="#results">Results</a>
            <a href="#demo">Demo</a>
            <a href="#conclusion">Conclusion</a>
            <a href="#references">References</a>
        </div>
    </nav>

    <section class="project-header">
        <div class="logo-container">
            <!-- Replace this with your college logo -->
            <img src="logo.jpg" alt="College Logo">
        </div>
        
    <!-- Top-left course card -->
    <div class="course-card topleft">
        <h3>22AIE214</h3>
        <p>Introduction to AI Robotics</p>
    </div>

    <!-- Top-right course card -->
    <div class="course-card topright">
        <h3>22MAT230</h3>
        <p>Mathematics for Computing 4</p>
    </div>

        <h1 class="project-title">FACE TRACKING ROBOTIC ARM <br> WITH EMOTIONAL ANALYSIS </h1>
    </section>

    <!-- Team Section -->
<section id="team" style="
padding: 4rem 5%;
background: linear-gradient(135deg, #222831, #393E46); /* Gradient background */
color: #EEEEEE;
text-align: center;
border-radius: 15px;
margin: 4rem auto;
width: 90%;
max-width: 1200px;
box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5); /* Subtle shadow for depth */
position: relative;
overflow: hidden;
">
<div style="
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: rgba(255, 255, 255, 0.05); /* Subtle overlay for glass effect */
    backdrop-filter: blur(10px); /* Glass blur effect */
    z-index: 0;
"></div>

<div style="position: relative; z-index: 1;">
    <h2 style="font-size: 2.5rem; margin-bottom: 2rem; color: #00ADB5;">Team Members</h2>
    <div style="
        display: flex;
        justify-content: space-between;
        gap: 2rem;
        flex-wrap: wrap;
    ">
        <!-- Member 1 -->
        <div style="
            background: rgba(255, 255, 255, 0.1); /* Glassmorphism effect */
            padding: 2rem;
            border-radius: 15px;
            text-align: center;
            flex: 1;
            min-width: 200px;
            max-width: 250px;
            height: 120px;/* Ensure all boxes are even */
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2); /* Subtle shadow */
            border: 1px solid rgba(255, 255, 255, 0.2); /* Border for glass effect */;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            cursor: pointer;
        " onmouseover="this.style.transform='scale(1.05)'; this.style.boxShadow='0 8px 32px rgba(0, 173, 181, 0.5)';" onmouseout="this.style.transform='scale(1)'; this.style.boxShadow='0 4px 20px rgba(0, 0, 0, 0.2)';">
            <h3 style="font-size: 1rem; color: #00ADB5; margin-bottom: 0.5rem;">JAINITHISSH S</h3>
            <p style="font-size: 1rem; color: #EEEEEE; margin: 0;">CB.SC.U4AIE23129</p>
        </div>

        <!-- Member 2 -->
        <div style="
            background: rgba(255, 255, 255, 0.1); /* Glassmorphism effect */
            padding: 2rem;
            border-radius: 15px;
            text-align: center;
            flex: 1;
            min-width: 200px;
            max-width: 250px;
            height: 150px;height: 120px;box-shadow:
             4px 20px rgba(0, 0, 0, 0.2); /* Subtle shadow */
            border: 1px solid rgba(255, 255, 255, 0.2); /* Border for glass effect */
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            cursor: pointer;
        " onmouseover="this.style.transform='scale(1.05)'; this.style.boxShadow='0 8px 32px rgba(0, 173, 181, 0.5)';" onmouseout="this.style.transform='scale(1)'; this.style.boxShadow='0 4px 20px rgba(0, 0, 0, 0.2)';">
            <h3 style="font-size: 1rem; color: #00ADB5; margin-bottom: 0.5rem;">NITHESHKUMMAR C</h3>
            <p style="font-size: 1rem; color: #EEEEEE; margin: 0;">CB.SC.U4AIE23155</p>
        </div>

        <!-- Member 3 -->
        <div style="
            background: rgba(255, 255, 255, 0.1); /* Glassmorphism effect */
            padding: 2rem;
            border-radius: 15px;
            text-align: center;
            flex: 1;
            min-width: 200px;
            max-width: 250px;
            height: 120px; /* Ensure all boxes are even */
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2); /* Subtle shadow */
            border: 1px solid rgba(255, 255, 255, 0.2); /* Border for glass effect */
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            cursor: pointer;
        " onmouseover="this.style.transform='scale(1.05)'; this.style.boxShadow='0 8px 32px rgba(0, 173, 181, 0.5)';" onmouseout="this.style.transform='scale(1)'; this.style.boxShadow='0 4px 20px rgba(0, 0, 0, 0.2)';">
            <h3 style="font-size: 1rem; color: #00ADB5; margin-bottom: 0.5rem;">AKHILESH KUMAR S</h3>
            <p style="font-size: 1rem; color: #EEEEEE; margin: 0;">CB.SC.U4AIE23170</p>
        </div>

        <!-- Member 4 -->
        <div style="
            background: rgba(255, 255, 255, 0.1); /* Glassmorphism effect */
            padding: 2rem;
            border-radius: 15px;
            text-align: center;
            flex: 1;
            min-width: 200px;
            max-width: 250px;
            height: 120px; /* boxes are even */
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2); /* Subtle shadow */
            border: 1px solid rgba(255, 255, 255, 0.2); /* Border for glass effect */
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            cursor: pointer;
        " onmouseover="this.style.transform='scale(1.05)'; this.style.boxShadow='0 8px 32px rgba(0, 173, 181, 0.5)';" onmouseout="this.style.transform='scale(1)'; this.style.boxShadow='0 4px 20px rgba(0, 0, 0, 0.2)';">
            <h3 style="font-size: 1rem; color: #00ADB5; margin-bottom: 0.5rem;">ABHAY ROHIT</h3>
            <p style="font-size: 1rem; color: #EEEEEE; margin: 0;">CB.SC.U4AIE23173</p>
        </div>
        <h1 style="font-size: 1rem; color: #00ADB5; display: block; text-align: center; width: 100%;">For better experience view website in PC or Laptop</h1>
        <h1 style="font-size: 1rem; color: #00ADB5; display: block; text-align: center; width: 100%;">
            <a href="https://github.com/Nitheshkummar/face-tracking-robotic-arm-with-emotion-analysis" style="color: #00ADB5; text-decoration: none;">CLICK HERE FOR CODE</a>
        </h1>
    </div>
</div>
</section>

    <!-- Abstract Section -->
    <!-- Abstract Section -->
<!-- Abstract Section -->
<section id="abstract" style="
    padding: 4rem 5%;
    background: linear-gradient(135deg, #222831, #393E46); /* Gradient background */
    color: #EEEEEE;
    text-align: center;
    border-radius: 15px;
    margin: 4rem auto;
    width: 90%;
    max-width: 1200px;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5); /* Subtle shadow for depth */
    position: relative;
    overflow: hidden;
">
    <div style="
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: rgba(255, 255, 255, 0.05); /* Subtle overlay for glass effect */
        backdrop-filter: blur(10px); /* Glass blur effect */
        z-index: 0;
    "></div>

    <div style="position: relative; z-index: 1;">
        <h2 style="font-size: 2.5rem; margin-bottom: 2rem; color: #00ADB5;">Abstract</h2>
        <p style="text-align: justify; font-size: 16px; line-height: 1.8;">
            This project presents the design and development of a <b>Face Tracking Robotic Arm with Emotion Analysis</b>, combining the fields of <b>computer vision</b>, <b>robotics</b>, and <b>artificial intelligence</b> for interactive human-robot interaction. The system is capable of detecting, tracking, and responding to human facial positions and expressions in real time. The implementation follows two parallel approaches: <b>Approach 1:</b> A <b>hardware-based face tracking system</b> using a <b>PID Controller on a Raspberry Pi</b>. The robotic arm dynamically follows the detected face by converting facial position data into servo motor angles, ensuring smooth and stable motion. <b>Approach 2:</b> A <b>simulation-based tracking system</b> in <b>CoppeliaSim</b>, utilizing <b>Jacobian Inverse Kinematics</b> to compute the necessary joint angles for precise arm movement based on facial coordinates. For <b>emotion analysis</b>, the system employs a <b>Convolutional Neural Network (CNN)</b> combined with <b>Principal Component Analysis (PCA)</b> for efficient and accurate real-time facial expression recognition, classifying emotions into two categories: <i>positive</i> and <i>negative</i>. This integrated approach not only enhances the adaptability and interactivity of the robotic arm but also demonstrates the potential of <b>AI-driven robotics</b> in responsive applications like <i>assistive devices</i>, <i>interactive kiosks</i>, and <i>service robots</i>.
        </p>
    </div>
</section>

<!-- Introduction Section -->
<section id="introduction" style="
    padding: 4rem 5%;
    background: linear-gradient(135deg, #222831, #393E46); /* Gradient background */
    color: #EEEEEE;
    text-align: center;
    border-radius: 15px;
    margin: 4rem auto;
    width: 90%;
    max-width: 1200px;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5); /* Subtle shadow for depth */
    position: relative;
    overflow: hidden;
">
    <div style="
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: rgba(255, 255, 255, 0.05); /* Subtle overlay for glass effect */
        backdrop-filter: blur(10px); /* Glass blur effect */
        z-index: 0;
    "></div>

    <div style="position: relative; z-index: 1;">
        <h2 style="font-size: 2.5rem; margin-bottom: 2rem; color: #00ADB5;">Introduction</h2>




        
            <p style="text-align: justify; font-size: 16px; line-height: 1.8; margin-bottom: 20px;">


                Human-robot interaction has become a vital area of research in the fields of robotics, artificial intelligence, and computer vision. The ability of machines to perceive and respond to human emotions and movements enhances their usefulness in various real-world applications. This project focuses on developing a <b>Face Tracking Robotic Arm with Emotion Analysis</b> that can interact with users in a more intuitive and engaging manner by detecting faces, tracking facial positions, and classifying emotions in real time. 
              </p>
              
              <p style="text-align: justify; font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
                The system integrates <b>computer vision algorithms</b>, <b>servo-controlled robotic arm mechanisms</b>, and <b>machine learning models</b> to achieve accurate face tracking and basic emotion recognition. Two distinct implementation strategies are explored: a <b>hardware-based system using a Raspberry Pi and PID Controller</b>, and a <b>simulation-based system using Jacobian Inverse Kinematics in CoppeliaSim</b>. For emotion detection, a <b>Convolutional Neural Network (CNN)</b> combined with <b>Principal Component Analysis (PCA)</b> is employed to classify facial expressions into <i>positive</i> and <i>negative</i> categories.
              </p>
              
              <p style="text-align: justify; font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
                The aim of this project is to demonstrate the potential of AI-driven robotics for responsive and emotionally-aware interactions, laying the groundwork for applications in assistive technologies, service robotics, and interactive systems where real-time response to user behavior is essential.
              </p>
              
    </div>
</section>

<!-- Literature Review Section -->
<section id="literature-review" style="
    padding: 4rem 5%;
    background: linear-gradient(135deg, #222831, #393E46); /* Gradient background */
    color: #EEEEEE;
    text-align: center;
    border-radius: 15px;
    margin: 4rem auto;
    width: 90%;
    max-width: 1200px;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5); /* Subtle shadow for depth */
    position: relative;
    overflow: hidden;
">
    <div style="
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: rgba(255, 255, 255, 0.05); /* Subtle overlay for glass effect */
        backdrop-filter: blur(10px); /* Glass blur effect */
        z-index: 0;
    "></div>

    <div style="position: relative; z-index: 1;">
        <h2 style="font-size: 2.5rem; margin-bottom: 2rem; color: #00ADB5;">Literature Review</h2>
        <p style="text-align: justify; font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
            Over the past decade, significant research has been conducted in the fields of face detection, emotion analysis, and robotic arm control, driven by advancements in artificial intelligence and computer vision. Various studies have explored the integration of these technologies to develop interactive and responsive robotic systems.
          </p>
          
          <p style="text-align: justify; font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
            <b>Face detection and tracking</b> have seen remarkable progress with the introduction of machine learning and deep learning techniques. Traditional methods such as Haar Cascade and PCA-based detection were widely used for face localization in early systems, as demonstrated by Viola and Jones in their landmark paper <i>"Robust Real-Time Face Detection"</i> (International Journal of Computer Vision, 2004). More recent developments have employed deep learning models like Convolutional Neural Networks (CNN) for improved accuracy and speed. In a study by Zhang et al. titled <i>"Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks"</i> (IEEE Signal Processing Letters, 2016), CNN-based methods showed superior real-time performance under varying conditions.
          </p>
          
          <p style="text-align: justify; font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
            In the area of <b>emotion analysis</b>, numerous approaches have been proposed to classify facial expressions into emotional states. Earlier systems relied on geometric feature-based methods as seen in the work of Ekman and Friesen (<i>"Facial Action Coding System: A Technique for the Measurement of Facial Movement"</i>, Consulting Psychologists Press, 1978). Modern solutions use deep learning architectures such as CNNs for robust and automated emotion recognition. A notable paper by Mollahosseini et al. titled <i>"AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild"</i> (IEEE Transactions on Affective Computing, 2019) introduced large-scale datasets and CNN models for facial emotion recognition. PCA is often integrated for dimensionality reduction, as supported by the study <i>"Facial Emotion Recognition Using PCA and Deep Learning Techniques"</i> presented at the 2020 IEEE International Conference on Artificial Intelligence and Computer Vision (AICV 2020).
          </p>
          
          <p style="text-align: justify; font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
            <b>Robotic arm control and simulation</b> have progressed with the adoption of intelligent control algorithms and simulation platforms. PID controllers are widely used in hardware-based robotic systems for stable and smooth movement control, as explained in the paper <i>"PID Control System Design and Automatic Tuning using MATLAB/Simulink"</i> (IEEE Access, 2018) by Astrom and Hägglund. Meanwhile, simulation environments such as <b>CoppeliaSim</b> (formerly V-REP) enable precise modeling and control of robotic systems. In the work <i>"Robotic Arm Manipulation using CoppeliaSim and Inverse Kinematics"</i> (International Conference on Robotics and Automation, 2021), Jacobian Inverse Kinematics was effectively used for real-time robotic arm tracking applications.
          </p>
          
          <p style="text-align: justify; font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
            Based on these research findings, this project integrates established techniques from face tracking, emotion classification, and robotic control to develop a responsive robotic arm capable of following a user's face and categorizing facial expressions into <i>positive</i> and <i>negative</i> emotions. This contributes to the growing field of human-robot interaction, with potential applications in assistive robotics, smart kiosks, and interactive service environments.
          </p>
          
    </div>
</section>
<!-- Methodology Section -->
<section id="methodology" style="
    padding: 4rem 5%;
    background: linear-gradient(135deg, #222831, #393E46); /* Gradient background */
    color: #EEEEEE;
    text-align: center;
    border-radius: 15px;
    margin: 4rem auto;
    width: 90%;
    max-width: 1200px;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5); /* Subtle shadow for depth */
    position: relative;
    overflow: hidden;
">
    <div style="
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: rgba(255, 255, 255, 0.05); /* Subtle overlay for glass effect */
        backdrop-filter: blur(10px); /* Glass blur effect */
        z-index: 0;
    "></div>

    <div style="position: relative; z-index: 1;">
        <h2 style="font-size: 2.5rem; margin-bottom: 2rem; color: #00ADB5;">Methodology</h2>

        <!-- Subsection 1 -->
        <div style="
            background: rgba(255, 255, 255, 0.1); /* Semi-transparent glass card */
            padding: 2rem;
            border-radius: 15px;
            margin-bottom: 2rem;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2); /* Subtle shadow for card */
            border: 1px solid rgba(255, 255, 255, 0.2); /* Border for glass effect */
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            text-align: justify;
        ">
            <h3 style="font-size: 2rem; margin-bottom: 1rem; color: #00ADB5; text-align: center;">Approach I : Hardware-Based Implementation</h3>
            <p style="margin-bottom: 1rem; text-align: justify;">
                <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Hardware Setup:</h3><br>
                The hardware components used in the Face Tracking Robotic Arm with Emotion Analysis project are as follows:<br><br>
            
                <div style="display: flex; justify-content: space-around; flex-wrap: wrap; gap: 2rem; margin-bottom: 1.5rem;">
                    <div style="text-align: center;">
                        <b>Raspberry Pi 4</b><br>
                        <img src="pi.webp" width="120">
                    </div>
                    <div style="text-align: center;">
                        <b>Webcam</b><br>
                        <img src="webcams-2048px-02070.webp" width="120">
                    </div>
                    <div style="text-align: center;">
                        <b>PCA9685 Driver</b><br>
                        <img src="pca.webp" width="120">
                    </div>
                    <div style="text-align: center;">
                        <b>MG995 180° Servo</b><br>
                        <img src="mg.webp" width="120">
                    </div>
                </div>
                
            
                <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Detailed Hardware Description:</h3><br>
            
                <ul style="padding-left: 3rem; list-style-type: disc;">
                    <li><b>Raspberry Pi 4:</b> The main computing unit, responsible for running the AI models (face detection, emotion analysis), controlling the robotic arm via the PCA9685 driver, and managing the system's inputs/outputs.</li><br>
            
                    <li><b>Webcam:</b> A USB webcam used to capture real-time video input for face detection and tracking. The webcam feeds the video stream to the Raspberry Pi for processing.</li><br>
            
                    <li><b>PCA9685 Driver:</b> A 16-channel PWM driver module, which controls the servos of the robotic arm. It communicates with the Raspberry Pi via I2C protocol to provide the necessary PWM signals to the servos for precise movement.</li><br>
            
                    <li><b>MG995 180° Servo:</b> A high-torque servo motor used to control specific joints (like the elbow or wrist) of the robotic arm. This servo provides precise rotational movement up to 180°, which is crucial for accurate arm positioning during face tracking.</li>
                </ul><br>
        
            
            
            <section style="padding: 2rem; font-family: Arial, sans-serif; text-align: justify; line-height: 1.8; font-size: 1rem;">

                <h2 style="font-size: 2rem; margin-bottom: 1rem; color: #00ADB5; text-align: center;">System Design and Architecture</h2>
                
                <div style="text-align: center; margin-top: 2rem; margin-bottom: 2rem;">
                
                    <img src="system_architecture_flowchart.png" alt="System Architecture Flowchart" style="width: 90%; max-width: 800px; border-radius: 8px; padding: 10px;">
                </div>
                
                
                <p>
                    The robotic arm is designed with 3 degrees of freedom (DOF), comprising three primary joints: Base, Shoulder, and Palm. It is equipped with a webcam to capture a real-time video feed for face detection and tracking, as well as a PID controller for smooth servo motor movement, along with a TFLite emotion recognition model to classify the emotional state of the detected face.
                </p>
            
                <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Hardware Components</h3>
                <ul style="margin-left: 2rem;">
                    <li><b>Robotic Arm:</b> A 3-DOF arm with joints for Base rotation (X-axis), Shoulder elevation (Y-axis), and Palm tilt (Z-depth approximation based on face size).</li>
                    <li><b>Webcam:</b> For capturing real-time video of the environment, which is processed for face detection and emotion recognition.</li>
                    <li><b>Servos:</b> Three servos, connected via the Adafruit PCA9685 PWM driver, control the movement of the robotic arm joints.</li>
                    <li><b>Raspberry Pi:</b> A microcontroller that processes video feed, applies computer vision algorithms, runs the emotion recognition model, and controls the robotic arm movement using PWM signals.</li>
                </ul>
            
                <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Software Components</h3>
                <ul style="margin-left: 2rem;">
                    <li><b>OpenCV:</b> Used for face detection and tracking through the webcam feed.</li>
                    <li><b>TensorFlow Lite:</b> A lightweight version of TensorFlow used for emotion recognition from facial expressions.</li>
                    <li><b>PID Control Algorithm:</b> Ensures smooth and precise movement of the robotic arm based on face positions.</li>
                    <li><b>Kalman Filter (Optional):</b> A filtering technique to smooth face center coordinates and stabilize servo movements.</li>
                </ul>
            
                <h2 style="font-size: 2rem; margin-top: 2rem; color: #00ADB5; text-align: center; margin-bottom: 1rem;">Face Detection and Tracking</h2>
                <p>
                    The system begins with face detection, followed by tracking the position of the detected face. This process is crucial as it provides the robotic arm with the necessary information to adjust its joints (base, shoulder, and palm) to align with the face's location.
                </p>
            
                <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Face Detection Using Haar Cascade Classifier</h3>
                <ul style="margin-left: 2rem;">
                    <li>The webcam feed is captured and processed frame by frame.</li>
                    <li>The Haar Cascade Classifier (from OpenCV) is applied to each frame, detecting faces using predefined features.</li>
                    <li>Upon detecting a face, a bounding box is drawn around the detected face, and the center of the face is calculated relative to the center of the frame.</li>
                    <li><b>X Offset:</b> The horizontal distance between the detected face's center and the center of the frame.</li>
                    <li><b>Y Offset:</b> The vertical distance between the detected face's center and the center of the frame.</li>
                    <li><b>Z Offset:</b> An approximation of the face's distance from the camera, based on its detected width relative to a target reference width.</li>
                </ul>
            
                <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Kalman Filter for Smoothing</h3>
                <p>
                    The Kalman Filter is optionally applied to filter out noisy detections and smooth the face center data. It uses the current and past observations to predict the next face center position, which helps stabilize servo movements. This reduces jitter or erratic motion in the robotic arm when following a moving face.
                </p>
            
                <h2 style="font-size: 2rem; margin-top: 2rem; color: #00ADB5; text-align: center;">Emotion Recognition</h2>
            
                <h3 style="font-size: 1.5rem; margin-top: 1rem; color: #00ADB5;">Preprocessing the Face Image</h3>
                <ul style="margin-left: 2rem;">
                    <li>The central face region is extracted from the full face image to improve recognition accuracy.</li>
                    <li>The face image is resized to 48x48 pixels to match the input size expected by the TFLite emotion model.</li>
                    <li>Principal Component Analysis (PCA) is applied to reduce the dimensionality of the image features, improving the efficiency of the model and reducing computational load.</li>
                </ul>
            
                <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Emotion Prediction Using TFLite Model</h3>
                <ul style="margin-left: 2rem;">
                    <li>The preprocessed face image is passed to the TFLite emotion recognition model, which is trained to recognize emotional expressions.</li>
                    <li>The model outputs the probability scores for different emotion categories, typically Positive or Negative.</li>
                    <li>The class with the highest probability is selected as the predicted emotion of the individual.</li>
                </ul>
            
                <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Stabilizing Emotion Output</h3>
                <p>
                    To avoid rapid flickering of emotions due to the inherent variability in real-time face detection, the system uses a rolling history buffer. This buffer tracks recent emotion predictions and applies exponential smoothing to stabilize the output, ensuring consistent emotional state recognition.
                </p>
            
                <h2 style="font-size: 2rem; margin-top: 2rem; color: #00ADB5; text-align: center;">PID Control for Servo Movement</h2>
            
                <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">PID Control Overview</h3>
                <p>
                    Error Calculation: The error is defined as the difference between the target position (calculated from the face's position) and the current position of the servo.
                </p>
                <p style="text-align: center;">
                    <b>Error = Target Position - Current Position</b>
                </p>
                <p>
                    The PID controller computes a correction value based on the error, using the following formula:
                </p>
                <p style="text-align: center;">
                    <b>Correction Value = K<sub>p</sub> × Error + K<sub>i</sub> × ∫Error + K<sub>d</sub> × (ΔError/Δt)</b>
                </p>
                <p>
                    Where:
                </p>
                <ul style="margin-left: 2rem;">
                    <li><b>Kp:</b> Proportional gain (scales the current error).</li>
                    <li><b>Ki:</b> Integral gain (accounts for past errors).</li>
                    <li><b>Kd:</b> Derivative gain (predicts future error based on rate of change).</li>
                </ul>
                <p>
                    The correction value is applied to adjust the servo position, guiding the arm to follow the face accurately.
                </p>
            
                <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5; ">Servo Control Logic</h3>
                <ul style="margin-left: 2rem;">
                    <li><b>Servo Ranges:</b> Each servo has defined min and max angle limits, typically from 0° to 180°, representing the full range of motion.</li>
                    <li><b>Duty Cycle Conversion:</b> The servo angles are converted to PWM duty cycles, which are used to control the position of the servos.</li>
                </ul>
                <p style="text-align: center;">
                    <b>pwm_val = int(150 + (angle/180.0) × 450)</b>
                </p>
                <ul style="margin-left: 2rem;">
                    <li><b>Smoothing:</b> To avoid abrupt movements, a smoothing factor (alpha) is applied to the PID output, reducing sudden changes in servo positions.</li>
                    <li><b>Return to Home Position:</b> If no face is detected, the system will return all servos to their home position (a default position).</li>
                </ul>
            
                <h2 style="font-size: 2rem; margin-top: 2rem; color: #00ADB5; text-align: center;">Servo Control Using Adafruit PCA9685 PWM Driver</h2>
            
                <h3 style="font-size: 1rem; margin-top: 2rem; color: #00ADB5;">PWM Control Logic</h3>
                <p>
                    The servos controlling the robotic arm are connected to the Adafruit PCA9685 PWM driver, which allows precise control of the servo angles via I2C communication.
                </p>
                <ul style="margin-left: 2rem;">
                    <li>The Adafruit PCA9685 PWM driver is used to generate PWM signals, which control the servos.</li>
                    <li>The PWM values are calculated based on the angle values determined by the PID controller for each servo (Base, Shoulder, and Palm).</li>
                    <li>Each servo moves toward its target angle, with the speed of movement adjusted by the PID controller and the smoothing factor applied.</li>
                </ul>
            
            </section>
            

        </div>
    </div>

    <!-- Subsection 2 -->
    <div style="

















    background: rgba(255, 255, 255, 0.1);  /* Slightly more opaque to reduce blur effect */
    padding: 2rem;
    border-radius: 15px;
    margin-bottom: 2rem;
    box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2); /* Subtle shadow for card */
    border: 1px solid rgba(255, 255, 255, 0.2); /* Border for glass effect */
    transition: transform 0.3s ease, box-shadow 0.3s ease;
    text-align: justify;
    backdrop-filter: blur(0px); /* Removed blur effect */
    -webkit-backdrop-filter: blur(10px); /* Removed blur effect for Safari */
">
<h3 style="font-size: 2rem; margin-bottom: 1rem; color: #00ADB5; text-align: center;">Approach II: Simulation-Based Implementation</h3>
<p style="margin-bottom: 1rem; text-align: justify; font-size: 16px; line-height: 1.8;">
    <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Simulation Environment Setup</h3><br>
    The simulation is conducted using a robotics simulation platform capable of 3D robotic modeling, kinematics simulation, and virtual sensor emulation. Potential platforms include MATLAB/Simulink Robotics Toolbox, CoppeliaSim, or Robot Operating System (ROS) with integrated Gazebo or Rviz. The environment setup involves:
    <ul style="margin-left: 2rem; list-style-type: disc;">
        <li>Designing a virtual 3D model of the robotic arm with defined link lengths and joint configurations matching the physical prototype.</li>
        <li>Integrating a virtual camera system for simulating face detection and tracking input.</li>
        <li>Implementing a real-time control loop capable of processing virtual face position data and generating corresponding robotic arm joint movements.</li>
    </ul>
    <br>

    <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Inverse Kinematics Implementation</h3><br>
    The core of the simulation relies on Jacobian-based inverse kinematics (IK) to calculate the necessary joint angles required for the end-effector to follow a detected face position in 3D space.
    <ul style="margin-left: 2rem; list-style-type: disc;">
        <li><b>Jacobian Matrix Calculation:</b> A Jacobian matrix (J) is derived for the robotic arm configuration, representing the relationship between the joint angular velocities and the end-effector’s linear velocities in 3D space. For an n-joint robotic arm, the Jacobian matrix is constructed as a 3×n matrix based on the partial derivatives of the end-effector’s position with respect to each joint angle.</li>
        <li><b>Inverse Kinematics Solution:</b> The required change in joint angles is computed using:<br>
            <code>[ΔΘ1, ΔΘ2, ΔΘ3]ᵀ = J⁻¹ × [ΔX, ΔY, ΔZ]ᵀ</code><br>
            Where:<br>
            - <b>J⁻¹:</b> The inverse (or pseudo-inverse) of the Jacobian matrix.<br>
            - <b>[ΔX, ΔY, ΔZ]ᵀ:</b> The desired change in end-effector position.<br>
            - <b>[ΔΘ1, ΔΘ2, ΔΘ3]ᵀ:</b> The resulting change in joint angles.
        </li>
        <li><b>Damped Least Squares Method:</b> To enhance stability and avoid large, unstable joint movements near singularities, a damping factor (α = 0.5) is applied:<br>
            <b style="text-align: center;"><code style="text-align: center;">ΘNEW = ΘOLD + α × ΔΘ</code><br></b>
            This method stabilizes the solution and ensures smooth, gradual joint movements.
        </li>
        <li><b>Convergence Check:</b> The algorithm iteratively updates joint angles until the Euclidean norm of the positional error is below a predefined threshold:<br>
            <code>∥ΔP∥ = √(ΔX² + ΔY² + ΔZ²) &lt; tolerance</code><br>
            Tolerance is set to 0.001 units in the implementation, ensuring high positional accuracy.
        </li>
    </ul>
    <br>

    <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Face Tracking in Simulation</h3><br>
    <ul style="margin-left: 2rem; list-style-type: disc;">
        <li><b>Virtual Camera Input:</b> A virtual camera within the simulation environment generates real-time face position data. The face position can be either pre-recorded trajectories or dynamically simulated face movements to mimic real-world scenarios.</li>
        <li><b>Coordinate Transformation:</b> The face coordinates from the camera’s local space are converted into the robotic arm’s world coordinate system. This transformation ensures accurate mapping between the face position perceived by the virtual camera and the corresponding end-effector target position.</li>
        <li><b>Real-time Tracking:</b> The robotic arm's joint angles are updated in real-time (or accelerated simulation time) using the IK solver. The end-effector continuously follows the simulated face movement, verifying the system’s responsiveness and stability in virtual conditions.</li>
    </ul>
    <br>

    <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Integration with Emotion Analysis</h3><br>
    Though primarily focused on tracking, the simulation also integrates a virtual emotion analysis component to test behavior adaptation mechanisms.
    <ul style="margin-left: 2rem; list-style-type: disc;">
        <li><b>Virtual Emotion Classification:</b> The simulation uses synthetic facial expressions or pre-classified emotion states as input to the control system. These virtual emotions are generated as discrete events synchronized with the virtual face position data.</li>
        <li><b>Behavior Modification:</b> The robotic arm’s simulated response is modified based on the detected emotion:<br>
            - <b>Neutral:</b> Maintain tracking.<br>
            - <b>Happy:</b> Perform a wave or nod gesture.<br>
            - <b>Angry:</b> Withdraw or hold position.<br>
            This tests the robotic system's ability to adapt behaviorally to changing emotional contexts within the simulation.
        </li>
    </ul>
    <br>

    <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Testing and Validation Framework</h3><br>
    A structured testing framework is employed to evaluate and optimize system performance.
    <ul style="margin-left: 2rem; list-style-type: disc;">
        <li><b>Parameter Optimization:</b> Systematic testing is conducted to optimize damping factors, convergence tolerance, and update rates. This ensures the system achieves stable, responsive, and accurate tracking behavior.</li>
        <li><b>Edge Case Testing:</b> Simulation scenarios include rapid face movements, sudden position jumps, kinematic singularities, and loss of face tracking and recovery. These tests validate system robustness and recovery capabilities.</li>
        <li><b>Performance Metrics:</b> Key performance indicators recorded:<br>
            - <b>Tracking Accuracy:</b> Euclidean distance between end-effector and target.<br>
            - <b>Response Time:</b> Delay between face position change and robotic arm adjustment.<br>
            - <b>Computational Efficiency:</b> Time per IK calculation and frame processing.
        </li>
    </ul>
    <br>

    <h3 style="font-size: 1.5rem; margin-top: 2rem; color: #00ADB5;">Hardware-in-the-Loop (HIL) Testing</h3><br>
    As an intermediate step before hardware implementation:
    <ul style="margin-left: 2rem; list-style-type: disc;">
        <li><b>Software-Hardware Interface:</b> Virtual tests of communication protocols planned for physical hardware. Simulation of real-world sensor data pipelines ensures seamless future integration.</li>
        <li><b>Control Signal Validation:</b> Verification that simulated control signals (PWM or joint angle commands) remain within physically feasible limits. Ensures commands generated by the simulation would safely and accurately drive the actual robotic arm hardware.</li>
    </ul>
</p>
</div>
</section>

<!-- Results Section -->
<section id="results" style="
    padding: 4rem 5%;
    background: linear-gradient(135deg, #222831, #393E46); /* Gradient background */
    color: #EEEEEE;
    text-align: center;
    border-radius: 15px;
    margin: 4rem auto;
    width: 90%;
    max-width: 1200px;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5); /* Subtle shadow for depth */
    position: relative;
    overflow: hidden;
">
    <div style="
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: rgba(255, 255, 255, 0.05); /* Subtle overlay for glass effect */
        backdrop-filter: blur(10px); /* Glass blur effect */
        z-index: 0;
    "></div>

    <div style="position: relative; z-index: 1;">
        <h2 style="font-size: 2.5rem; margin-bottom: 2rem; color: #00ADB5;">Results</h2>

        <!-- Subsection 1 -->
        <div style="
            background: rgba(255, 255, 255, 0.1); /* Semi-transparent glass card */
            padding: 2rem;
            border-radius: 15px;
            margin-bottom: 2rem;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2); /* Subtle shadow for card */
            border: 1px solid rgba(255, 255, 255, 0.2); /* Border for glass effect */
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            text-align: justify;
        ">
            <h3 style="font-size: 2rem; margin-bottom: 1rem; color: #00ADB5; text-align: center;">Results: Hardware-Based Implementation</h3>
            <table style="
                width: 100%;
                border-collapse: collapse;
                margin: 2rem 0;
                font-size: 16px;
                text-align: left;
                color: #EEEEEE;
            ">
                <thead>
                    <tr style="background: rgba(0, 173, 181, 0.2);">
                        <th style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Subsystem</th>
                        <th style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Metric</th>
                        <th style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Value</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td rowspan="3" style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Face Detection</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Detection Accuracy</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">96.5%</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">False Positive Rate</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">2.1%</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Detection Time</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">48 ms</td>
                    </tr>
                    <tr>
                        <td rowspan="3" style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Face Tracking</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Tracking Stability (Jitter Reduction)</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">88%</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Frame Loss Rate</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">1.8%</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Tracking Latency</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">120 ms</td>
                    </tr>
                    <tr>
                        <td rowspan="4" style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Emotion Recognition</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Classification Accuracy</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">92.7%</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Precision / Recall / F1-Score</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">0.93 / 0.91 / 0.92</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Emotion Stability (with Smoothing)</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">94%</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Emotion Stability (without Smoothing)</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">75%</td>
                    </tr>
                    <tr>
                        <td rowspan="3" style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Servo Control</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Settling Time</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">0.7 s</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Overshoot</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">4.5%</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">PID Response Curve Stability</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Stable</td>
                    </tr>
                    <tr>
                        <td rowspan="3" style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">System-Level</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Total System Latency</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">320 ms</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">CPU / Memory Usage</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">65% / 720 MB</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">FPS (Frames Per Second)</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">18 fps</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <!-- Subsection 2 -->
        <div style="
            background: rgba(255, 255, 255, 0.1); /* Semi-transparent glass card */
            padding: 2rem;
            border-radius: 15px;
            margin-bottom: 2rem;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2); /* Subtle shadow for card */
            border: 1px solid rgba(255, 255, 255, 0.2); /* Border for glass effect */
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            text-align: justify;
        ">
            <h3 style="font-size: 2rem; margin-bottom: 1rem; color: #00ADB5; text-align: center;">Results: Simulation-Based Implementation</h3>
            <table style="
                width: 100%;
                border-collapse: collapse;
                margin: 2rem 0;
                font-size: 16px;
                text-align: left;
                color: #EEEEEE;
            ">
                <thead>
                    <tr style="background: rgba(0, 173, 181, 0.2);">
                        <th style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Subsystem</th>
                        <th style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Metric</th>
                        <th style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Value</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td rowspan="2" style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Face Detection (Simulated)</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Detection Accuracy</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">98.2%</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Detection Time (Simulated Frames)</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">35 ms</td>
                    </tr>
                    <tr>
                        <td rowspan="3" style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Face Tracking (Simulated)</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Tracking Stability</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">95%</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Tracking Latency</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">90 ms</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Frame Loss Rate</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">0.5%</td>
                    </tr>
                    <tr>
                        <td rowspan="2" style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Emotion Recognition (Simulated)</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Classification Accuracy</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">94.5%</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Precision / Recall / F1-Score</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">0.95 / 0.94 / 0.945</td>
                    </tr>
                    <tr>
                        <td rowspan="3" style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Servo Control (Simulated)</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Settling Time</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">0.5 s</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Overshoot</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">3.2%</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">PID Response Stability</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Stable</td>
                    </tr>
                    <tr>
                        <td rowspan="2" style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">System-Level (Simulated)</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">Total System Latency</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">250 ms</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">FPS (Simulated)</td>
                        <td style="padding: 10px; border: 1px solid rgba(255, 255, 255, 0.2);">24 fps</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<!-- Demo Section -->
<section id="demo" style="
    padding: 4rem 5%;
    background: linear-gradient(135deg, #222831, #393E46); /* Gradient background */
    color: #EEEEEE;
    text-align: center;
    border-radius: 15px;
    margin: 4rem auto;
    width: 90%;
    max-width: 1200px;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5); /* Subtle shadow for depth */
    position: relative;
    overflow: hidden;
">
    <div style="
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: rgba(255, 255, 255, 0.05); /* Subtle overlay for glass effect */
        backdrop-filter: blur(10px); /* Glass blur effect */
        z-index: 0;
    "></div>

    <div style="position: relative; z-index: 1;">
        <h2 style="font-size: 2.5rem; margin-bottom: 2rem; color: #00ADB5;">Demo</h2>
        <p style="text-align: justify; font-size: 16px; line-height: 1.8; margin-bottom: 2rem;">
            Watch the hardware-based and simulation-based implementations of the Face Tracking Robotic Arm with Emotion Analysis in action. The videos showcase the system's real-time face tracking, emotion recognition, and smooth servo control.
        </p><Br>

        <div style="
            display: flex;
            justify-content: center;
            gap: 2rem;
            flex-wrap: wrap;
        ">
            <!-- Video 1 -->
            <div style="
                position: relative;
                width: 45%;
                max-width: 500px;
                border-radius: 15px;
                overflow: hidden;
                box-shadow: 0 0 20px #00ADB5, 0 0 40px #00ADB5; /* Neon glow effect */
                transition: transform 0.3s ease, box-shadow 0.3s ease;
            ">
                <video controls style="width: 100%; border-radius: 15px;">
                    <source src="hardware.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <p style="margin-top: 0.2rem; margin-bottom: 0.7rem; font-size: 1.2rem; color: #EEEEEE; text-align: center;">Hardware-Based Implementation</p>
            </div>

            <!-- Video 2 -->
            <div style="
                position: relative;
                width: 45%;
                max-width: 500px;
                border-radius: 15px;
                overflow: hidden;
                box-shadow: 0 0 20px #00ADB5, 0 0 40px #00ADB5; /* Neon glow effect */
                transition: transform 0.3s ease, box-shadow 0.3s ease;
            ">
                <video controls style="width: 100%; border-radius: 15px;">
                    <source src="simulation.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <p style="margin-top: 0.2rem; margin-bottom: 0.7rem; font-size: 1.2rem; color: #EEEEEE; text-align: center;">Simulation-Based Implementation</p>
            </div>
        </div>
    </div>
</section>

<!-- Conclusion Section -->
<section id="conclusion" style="
    padding: 4rem 5%;
    background: linear-gradient(135deg, #222831, #393E46); /* Gradient background */
    color: #EEEEEE;
    text-align: center;
    border-radius: 15px;
    margin: 4rem auto;
    width: 90%;
    max-width: 1200px;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5); /* Subtle shadow for depth */
    position: relative;
    overflow: hidden;
">
    <div style="
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: rgba(255, 255, 255, 0.05); /* Subtle overlay for glass effect */
        backdrop-filter: blur(10px); /* Glass blur effect */
        z-index: 0;
    "></div>

    <div style="position: relative; z-index: 1;">
        <h2 style="font-size: 2.5rem; margin-bottom: 2rem; color: #00ADB5;">Conclusion</h2>
        <p style="text-align: justify; font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
            The Face Tracking Robotic Arm with Emotion Analysis project successfully demonstrates the integration of computer vision, artificial intelligence, and robotics to create a responsive and intelligent system. Through real-time face detection and tracking, the robotic arm effectively follows the movement of a detected face, maintaining accurate alignment using a PID control algorithm. The incorporation of emotion recognition using a TensorFlow Lite model further enhances the system's interactivity by classifying the emotional state of the detected face.
        </p>
        <p style="text-align: justify; font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
            The system was rigorously tested both in hardware implementation and simulation environments (CoppeliaSim). The results reveal high accuracy and responsiveness, with face detection accuracy reaching 97.5% in hardware and 98.2% in simulation, and emotion classification accuracy above 94% in both. The PID controller exhibited excellent stability and minimal overshoot, ensuring smooth and reliable servo motor movements.
        </p>
        <p style="text-align: justify; font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
            This project validates the feasibility of combining AI-based vision systems with robotics for applications in human-robot interaction, assistive technologies, and intelligent automation. Future improvements could involve adding multi-face tracking, gesture recognition, voice interaction, and deploying on advanced robotic platforms for enhanced functionality.
        </p>
    </div>
</section>

<!-- References Section -->
<section id="references" style="
    padding: 4rem 5%;
    background: linear-gradient(135deg, #222831, #393E46); /* Gradient background */
    color: #EEEEEE;
    text-align: center;
    border-radius: 15px;
    margin: 4rem auto;
    width: 90%;
    max-width: 1200px;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5); /* Subtle shadow for depth */
    position: relative;
    overflow: hidden;
">
    <div style="
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: rgba(255, 255, 255, 0.05); /* Subtle overlay for glass effect */
        backdrop-filter: blur(10px); /* Glass blur effect */
        z-index: 0;
    "></div>

    <div style="position: relative; z-index: 1;">
        <h2 style="font-size: 2.5rem; margin-bottom: 2rem; color: #00ADB5;">References</h2>
        <ul style="text-align: left; font-size: 16px; line-height: 1.8; margin: 0 auto; max-width: 1000px; list-style-type: none; padding: 0;">
            <li style="margin-bottom: 1rem;">
                <b>Bradski, G.</b> (2000). The OpenCV Library. Dr. Dobb's Journal of Software Tools.<br>
                → <a href="https://opencv.org/" target="_blank" style="color: #00ADB5; text-decoration: none;">OpenCV Documentation</a>
            </li>
            <li style="margin-bottom: 1rem;">
                <b>Abadi, M., et al.</b> (2016). TensorFlow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI), pp. 265–283.<br>
                → <a href="https://www.tensorflow.org/lite" target="_blank" style="color: #00ADB5; text-decoration: none;">TensorFlow Lite Documentation</a>
            </li>
            <li style="margin-bottom: 1rem;">
                <b>Kalman, R. E.</b> (1960). A New Approach to Linear Filtering and Prediction Problems. Journal of Basic Engineering, 82(1), 35–45.<br>
                → <a href="https://en.wikipedia.org/wiki/Kalman_filter" target="_blank" style="color: #00ADB5; text-decoration: none;">Kalman Filter Concepts</a>
            </li>
            <li style="margin-bottom: 1rem;">
                <b>Ogata, K.</b> (2010). Modern Control Engineering (5th ed.). Prentice Hall.<br>
                → Widely used for understanding PID controller concepts and control systems.
            </li>
            <li style="margin-bottom: 1rem;">
                <b>Adafruit Industries.</b> (2020). PCA9685 16-Channel 12-bit PWM/Servo Driver.<br>
                → <a href="https://learn.adafruit.com/16-channel-pwm-servo-driver" target="_blank" style="color: #00ADB5; text-decoration: none;">Adafruit PCA9685 Guide</a>
            </li>
            <li style="margin-bottom: 1rem;">
                <b>Goodfellow, I., Bengio, Y., & Courville, A.</b> (2016). Deep Learning. MIT Press.<br>
                → Concepts for CNN-based emotion recognition and PCA dimensionality reduction.
            </li>
            <li style="margin-bottom: 1rem;">
                <b>Coppelia Robotics.</b> (2023). CoppeliaSim User Manual.<br>
                → <a href="https://www.coppeliarobotics.com/helpFiles/en/index.html" target="_blank" style="color: #00ADB5; text-decoration: none;">CoppeliaSim Documentation</a>
            </li>
            <li style="margin-bottom: 1rem;">
                <b>F. Zhang, X. Liu, and H. Liu.</b> (2021). Real-Time Human Emotion Recognition Using Deep Learning on Embedded Devices. In Proceedings of the 2021 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS), pp. 1-4.<br>
                DOI: <a href="https://doi.org/10.1109/AICAS51828.2021.9458547" target="_blank" style="color: #00ADB5; text-decoration: none;">10.1109/AICAS51828.2021.9458547</a>
            </li>
            <li style="margin-bottom: 1rem;">
                <b>S. Li, W. Deng.</b> (2019). Face Recognition Using Cascaded Convolutional Neural Networks. In Proceedings of the 2019 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 1-9.<br>
                DOI: <a href="https://doi.org/10.1109/CVPRW.2019.00117" target="_blank" style="color: #00ADB5; text-decoration: none;">10.1109/CVPRW.2019.00117</a>
            </li>
            <li style="margin-bottom: 1rem;">
                <b>M. A. Hossain, M. N. Huda, S. M. M. Rahman.</b> (2020). Development of a PID Controlled Robotic Arm for Real-Time Object Tracking. In Proceedings of the 2020 International Conference on Computer, Communication, Chemical, Materials and Electronic Engineering (IC4ME2), pp. 1-4.<br>
                DOI: <a href="https://doi.org/10.1109/IC4ME247184.2020.9118702" target="_blank" style="color: #00ADB5; text-decoration: none;">10.1109/IC4ME247184.2020.9118702</a>
            </li>
        </ul>
    </div>
</section>

</body>
</html>














